{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f2539d-90f2-418f-8477-034cd50e6d77",
   "metadata": {},
   "source": [
    "text,label\n",
    "\"Almost done! just reviewing :)\",2\n",
    "\"That is unacceptable, fix it now!\",0\n",
    "\"Ok\",1\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a029b09-efe6-4fee-929e-f1a928074c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full advanced code (BERT fine-tune with custom loss option & metrics)\n",
    "\"\"\"\n",
    "Advanced BERT fine-tuning for WhatsApp sentiment classification.\n",
    "Supports:\n",
    " - Hugging Face Trainer-based training\n",
    " - Weighted loss or Focal Loss for class imbalance\n",
    " - Stratified train/val split\n",
    " - Evaluation metrics (precision, recall, f1)\n",
    " - Inference helper\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # swap for \"bert-base-uncased\" or \"roberta-base\"\n",
    "NUM_LABELS = 3  # negative, neutral, positive\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LR = 2e-5\n",
    "MAX_LEN = 128\n",
    "SEED = 42\n",
    "OUTPUT_DIR = \"./whatsapp_bert_finetuned\"\n",
    "USE_FOCAL_LOSS = False  # set True to use focal loss (helpful for imbalance)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------\n",
    "# Utilities\n",
    "# -----------------------\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# -----------------------\n",
    "# Load CSV and prep Dataset\n",
    "# -----------------------\n",
    "df = pd.read_csv(\"whatsapp_sentiment.csv\")  # expects columns text,label\n",
    "# quick cleaning suggestion (you can expand)\n",
    "df['text'] = df['text'].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.12, stratify=df['label'], random_state=SEED)\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "hf_val = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\"train\": hf_train, \"validation\": hf_val})\n",
    "\n",
    "# -----------------------\n",
    "# Tokenizer & Data Collator\n",
    "# -----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True, remove_columns=[\"text\", \"__index_level_0__\"], num_proc=1)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# -----------------------\n",
    "# Compute class weights (for WeightedLoss)\n",
    "# -----------------------\n",
    "label_counts = train_df['label'].value_counts().sort_index().values  # order: 0,1,2\n",
    "total = label_counts.sum()\n",
    "class_weights = [total / (len(label_counts) * c) if c > 0 else 0.0 for c in label_counts]\n",
    "class_weights = torch.tensor(class_weights).to(DEVICE)\n",
    "print(\"Label counts:\", label_counts, \"Class weights:\", class_weights.cpu().numpy())\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# -----------------------\n",
    "# Custom loss wrapper (if using weighted or focal loss)\n",
    "# -----------------------\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, use_focal=False, focal_gamma=2.0, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.use_focal = use_focal\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\").to(DEVICE)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        if self.use_focal:\n",
    "            # Focal loss\n",
    "            ce = torch.nn.functional.cross_entropy(logits, labels, reduction=\"none\", weight=self.class_weights)\n",
    "            p_t = torch.exp(-ce)  # p_t = exp(-CE)\n",
    "            focal = ((1 - p_t) ** self.focal_gamma) * ce\n",
    "            loss = focal.mean()\n",
    "        elif self.class_weights is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# -----------------------\n",
    "# Metrics\n",
    "# -----------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)\n",
    "    report = classification_report(labels, preds, zero_division=0, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "        \"report\": report\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# TrainingArguments\n",
    "# -----------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Instantiate CustomTrainer\n",
    "# -----------------------\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    use_focal=USE_FOCAL_LOSS,\n",
    "    focal_gamma=2.0,\n",
    "    class_weights=class_weights if not USE_FOCAL_LOSS else None\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Train\n",
    "# -----------------------\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------\n",
    "# Evaluate on validation set\n",
    "# -----------------------\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Validation metrics:\", metrics)\n",
    "\n",
    "# -----------------------\n",
    "# Save final model + tokenizer\n",
    "# -----------------------\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# -----------------------\n",
    "# Inference helper\n",
    "# -----------------------\n",
    "def predict_texts(texts, model_path=OUTPUT_DIR):\n",
    "    tok = AutoTokenizer.from_pretrained(model_path)\n",
    "    mod = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "    enc = tok(texts, truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_LEN).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = mod(**enc)\n",
    "        logits = out.logits\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        preds = np.argmax(probs, axis=-1)\n",
    "    return preds, probs\n",
    "\n",
    "# Example inference:\n",
    "samples = [\"Thanks, that helped a lot ðŸ˜„\", \"This is unacceptable, fix it now!\", \"Okay.\"]\n",
    "preds, probs = predict_texts(samples)\n",
    "print(list(zip(samples, preds, probs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
